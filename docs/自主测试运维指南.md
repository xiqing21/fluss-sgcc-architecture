# 🎯 Fluss 自主测试运维指南

## 📋 目录
1. [环境启动与检查](#环境启动与检查)
2. [场景独立执行](#场景独立执行)
3. [实时监控方法](#实时监控方法)
4. [数据变更计算](#数据变更计算)
5. [故障排查与数据补货](#故障排查与数据补货)
6. [性能调优技巧](#性能调优技巧)

---

## 🚀 环境启动与检查

### 1. 启动完整环境
```bash
# 启动所有服务
docker-compose up -d

# 检查服务状态
docker-compose ps

# 检查关键服务健康状态
docker exec postgres-sgcc-source pg_isready -U sgcc_user
docker exec postgres-sgcc-sink pg_isready -U sgcc_user
```

### 2. 验证核心组件
```bash
# 检查Fluss集群状态
curl -s http://localhost:9123/health

# 检查Flink Web UI (可选)
echo "访问 http://localhost:8091 查看Flink作业状态"

# 检查数据库连接
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "\dt"
docker exec postgres-sgcc-sink psql -U sgcc_user -d sgcc_dw_db -c "\dt"
```

---

## 🎮 场景独立执行

### 1. 创建执行脚本
```bash
# 创建场景执行目录
mkdir -p ~/fluss_scenarios

# 场景1：高频维度表服务
cat > ~/fluss_scenarios/run_scenario1.sh << 'EOF'
#!/bin/bash
echo "🚀 开始执行场景1：高频维度表服务"

# 执行场景SQL
docker exec sql-client-sgcc /opt/flink/bin/sql-client.sh \
    -f /opt/sql/场景1_高频维度表服务.sql

echo "✅ 场景1执行完成，请检查作业状态"
EOF

chmod +x ~/fluss_scenarios/run_scenario1.sh
```

### 2. 分段执行方式
```bash
# 方法1：完整执行
~/fluss_scenarios/run_scenario1.sh

# 方法2：分段执行（推荐）
# 第一步：创建表结构
cat > /tmp/step1_tables.sql << 'EOF'
SET 'sql-client.execution.result-mode' = 'TABLEAU';

-- 1. 创建CDC源表
CREATE TABLE device_raw_stream (
    device_id STRING,
    voltage DOUBLE,
    current_val DOUBLE,
    temperature DOUBLE,
    power_output DOUBLE,
    efficiency DOUBLE,
    status STRING,
    alert_level STRING,
    event_time TIMESTAMP(3),
    WATERMARK FOR event_time AS event_time - INTERVAL '1' SECOND
) WITH (
    'connector' = 'postgres-cdc',
    'hostname' = 'postgres-sgcc-source',
    'port' = '5432',
    'username' = 'sgcc_user',
    'password' = 'sgcc_pass_2024',
    'database-name' = 'sgcc_source_db',
    'schema-name' = 'public',
    'table-name' = 'device_raw_data',
    'slot.name' = 'device_raw_slot',
    'decoding.plugin.name' = 'pgoutput'
);

-- 2. 创建Fluss分层表
CREATE CATALOG fluss_catalog WITH (
    'type' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

USE CATALOG fluss_catalog;
CREATE DATABASE IF NOT EXISTS fluss;
USE fluss;

-- ODS层
CREATE TABLE IF NOT EXISTS ods_device_raw (
    device_id STRING PRIMARY KEY NOT ENFORCED,
    voltage DOUBLE,
    current_val DOUBLE,
    temperature DOUBLE,
    power_output DOUBLE,
    efficiency DOUBLE,
    status STRING,
    alert_level STRING,
    event_time TIMESTAMP(3)
) WITH (
    'connector' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);
EOF

# 执行表创建
docker cp /tmp/step1_tables.sql sql-client-sgcc:/opt/sql/
docker exec sql-client-sgcc /opt/flink/bin/sql-client.sh -f /opt/sql/step1_tables.sql
```

---

## 📊 实时监控方法

### 1. 作业状态监控
```bash
# 创建监控脚本
cat > ~/fluss_scenarios/monitor.sh << 'EOF'
#!/bin/bash

echo "=== 🔍 Fluss集群监控 ==="
echo "当前时间: $(date)"
echo

# 1. 检查Flink作业状态
echo "📋 Flink作业列表:"
docker exec jobmanager-sgcc /opt/flink/bin/flink list 2>/dev/null | grep -E "(RUNNING|FAILED|FINISHED)"

echo
echo "📊 Docker容器状态:"
docker-compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}"

echo
echo "💾 数据库连接测试:"
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "SELECT COUNT(*) FROM device_raw_data;" 2>/dev/null || echo "❌ 源数据库连接失败"
docker exec postgres-sgcc-sink psql -U sgcc_user -d sgcc_dw_db -c "SELECT COUNT(*) FROM device_final_report;" 2>/dev/null || echo "❌ 目标数据库连接失败"

EOF

chmod +x ~/fluss_scenarios/monitor.sh
```

### 2. 数据流监控
```bash
# 创建数据流监控脚本
cat > ~/fluss_scenarios/data_monitor.sql << 'EOF'
SET 'sql-client.execution.result-mode' = 'TABLEAU';

CREATE CATALOG fluss_catalog WITH (
    'type' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 查看各层数据量（快照）
SELECT 'ODS层' as layer, COUNT(*) as records FROM fluss_catalog.fluss.ods_device_raw;
EOF

# 定期执行数据监控
watch -n 10 'docker cp ~/fluss_scenarios/data_monitor.sql sql-client-sgcc:/opt/sql/ && timeout 5 docker exec sql-client-sgcc /opt/flink/bin/sql-client.sh -f /opt/sql/data_monitor.sql 2>/dev/null | tail -5'
```

### 3. Web UI监控
```bash
echo "🌐 访问以下URL进行可视化监控:"
echo "- Flink Web UI: http://localhost:8091"
echo "- 作业监控: http://localhost:8091/#/job/running"
echo "- Metrics监控: http://localhost:8091/#/job/running/{job-id}/metrics"
```

---

## 🔄 数据变更计算

### 1. 实时变更监控
```sql
-- 创建变更监控表
CREATE TABLE change_log_monitor (
    device_id STRING,
    old_efficiency DOUBLE,
    new_efficiency DOUBLE,
    change_type STRING,  -- INSERT, UPDATE, DELETE
    change_time TIMESTAMP(3),
    PRIMARY KEY (device_id, change_time) NOT ENFORCED
) WITH (
    'connector' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 监控效率变更超过阈值的设备
INSERT INTO change_log_monitor
SELECT 
    device_id,
    LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time) as old_efficiency,
    efficiency as new_efficiency,
    'UPDATE' as change_type,
    event_time as change_time
FROM fluss_catalog.fluss.ods_device_raw
WHERE ABS(efficiency - LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time)) > 0.05;
```

### 2. 增量计算逻辑
```bash
# 创建增量计算脚本
cat > ~/fluss_scenarios/incremental_compute.sql << 'EOF'
SET 'sql-client.execution.result-mode' = 'TABLEAU';

CREATE CATALOG fluss_catalog WITH (
    'type' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 创建增量汇总表
CREATE TABLE IF NOT EXISTS incremental_summary (
    summary_id STRING PRIMARY KEY NOT ENFORCED,
    window_start TIMESTAMP(3),
    window_end TIMESTAMP(3),
    device_count BIGINT,
    avg_efficiency DOUBLE,
    efficiency_change_rate DOUBLE,
    alert_count BIGINT,
    compute_time TIMESTAMP(3)
) WITH (
    'connector' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 5分钟滚动窗口增量计算
INSERT INTO fluss_catalog.fluss.incremental_summary
SELECT 
    CONCAT('INCR_', DATE_FORMAT(window_start, 'yyyyMMdd_HHmmss')) as summary_id,
    window_start,
    window_end,
    COUNT(DISTINCT device_id) as device_count,
    AVG(efficiency) as avg_efficiency,
    (MAX(efficiency) - MIN(efficiency)) / MIN(efficiency) as efficiency_change_rate,
    COUNT(CASE WHEN alert_level = 'H' THEN 1 END) as alert_count,
    CURRENT_TIMESTAMP as compute_time
FROM TABLE(
    TUMBLE(TABLE fluss_catalog.fluss.ods_device_raw, 
           DESCRIPTOR(event_time), 
           INTERVAL '5' MINUTE)
)
GROUP BY window_start, window_end;
EOF
```

### 3. 变更触发器
```bash
# 创建变更响应脚本
cat > ~/fluss_scenarios/change_trigger.sql << 'EOF'
-- 异常变更自动响应
CREATE TABLE alert_response (
    alert_id STRING PRIMARY KEY NOT ENFORCED,
    device_id STRING,
    alert_type STRING,
    efficiency_drop DOUBLE,
    response_action STRING,
    trigger_time TIMESTAMP(3)
) WITH (
    'connector' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 效率下降超过10%自动触发告警
INSERT INTO alert_response
SELECT 
    CONCAT('ALERT_', device_id, '_', UNIX_TIMESTAMP()) as alert_id,
    device_id,
    'EFFICIENCY_DROP' as alert_type,
    ABS(efficiency - LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time)) as efficiency_drop,
    CASE 
        WHEN ABS(efficiency - LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time)) > 0.15 
        THEN 'IMMEDIATE_MAINTENANCE'
        WHEN ABS(efficiency - LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time)) > 0.10 
        THEN 'SCHEDULE_INSPECTION'
        ELSE 'MONITOR_CLOSELY'
    END as response_action,
    event_time as trigger_time
FROM fluss_catalog.fluss.ods_device_raw
WHERE ABS(efficiency - LAG(efficiency) OVER (PARTITION BY device_id ORDER BY event_time)) > 0.10;
EOF
```

---

## 🔧 故障排查与数据补货

### 1. 常见故障诊断
```bash
# 创建故障诊断脚本
cat > ~/fluss_scenarios/diagnose.sh << 'EOF'
#!/bin/bash

echo "🔍 开始故障诊断..."

# 1. 检查作业状态
echo "=== Flink作业状态 ==="
docker exec jobmanager-sgcc /opt/flink/bin/flink list | grep -E "(FAILED|RESTARTING|CANCELED)"

# 2. 检查容器日志
echo "=== 容器错误日志 ==="
docker logs coordinator-server-sgcc --tail 20 2>&1 | grep -i error
docker logs jobmanager-sgcc --tail 20 2>&1 | grep -i error

# 3. 检查数据库连接
echo "=== 数据库连接测试 ==="
docker exec postgres-sgcc-source pg_isready -U sgcc_user || echo "❌ 源数据库连接失败"
docker exec postgres-sgcc-sink pg_isready -U sgcc_user || echo "❌ 目标数据库连接失败"

# 4. 检查CDC槽位状态
echo "=== CDC复制槽状态 ==="
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "SELECT slot_name, active, restart_lsn FROM pg_replication_slots;"

echo "✅ 诊断完成"
EOF

chmod +x ~/fluss_scenarios/diagnose.sh
```

### 2. 数据补货策略
```bash
# 创建数据补货脚本
cat > ~/fluss_scenarios/data_recovery.sh << 'EOF'
#!/bin/bash

echo "🔄 开始数据补货..."

# 1. 停止相关作业
echo "停止现有作业..."
docker exec jobmanager-sgcc /opt/flink/bin/flink list | grep RUNNING | awk '{print $4}' | while read job_id; do
    docker exec jobmanager-sgcc /opt/flink/bin/flink cancel $job_id
done

# 2. 清理目标表（可选）
echo "清理目标表..."
docker exec postgres-sgcc-sink psql -U sgcc_user -d sgcc_dw_db -c "TRUNCATE TABLE device_final_report;"

# 3. 重置CDC槽位
echo "重置CDC槽位..."
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "SELECT pg_drop_replication_slot('device_raw_slot');" || true
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "SELECT * FROM pg_create_logical_replication_slot('device_raw_slot', 'pgoutput');"

# 4. 重新启动数据流
echo "重新启动数据流..."
docker exec sql-client-sgcc /opt/flink/bin/sql-client.sh -f /opt/sql/场景1_高频维度表服务.sql

echo "✅ 数据补货完成"
EOF

chmod +x ~/fluss_scenarios/data_recovery.sh
```

### 3. 历史数据回放
```sql
-- 历史数据回放脚本
CREATE TABLE historical_replay (
    device_id STRING,
    voltage DOUBLE,
    efficiency DOUBLE,
    replay_timestamp TIMESTAMP(3),
    original_timestamp TIMESTAMP(3),
    PRIMARY KEY (device_id, replay_timestamp) NOT ENFORCED
) WITH (
    'connector' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

-- 回放指定时间段的数据
INSERT INTO historical_replay
SELECT 
    device_id,
    voltage,
    efficiency,
    CURRENT_TIMESTAMP as replay_timestamp,
    event_time as original_timestamp
FROM fluss_catalog.fluss.ods_device_raw
WHERE event_time BETWEEN '2024-01-01 00:00:00' AND '2024-01-01 23:59:59';
```

---

## ⚡ 性能调优技巧

### 1. 调优参数配置
```bash
# 创建调优配置
cat > ~/fluss_scenarios/performance_tuning.sql << 'EOF'
-- 设置性能优化参数
SET 'execution.checkpointing.interval' = '30s';
SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';
SET 'state.backend' = 'rocksdb';
SET 'state.checkpoints.dir' = 'file:///opt/flink/checkpoints';
SET 'parallelism.default' = '4';
SET 'table.exec.resource.default-parallelism' = '4';
SET 'table.exec.mini-batch.enabled' = 'true';
SET 'table.exec.mini-batch.allow-latency' = '1s';
SET 'table.exec.mini-batch.size' = '1000';
EOF
```

### 2. 监控关键指标
```bash
# 创建性能监控脚本
cat > ~/fluss_scenarios/performance_monitor.sh << 'EOF'
#!/bin/bash

echo "📈 性能监控报告"
echo "时间: $(date)"
echo

# 1. 内存使用情况
echo "=== 内存使用 ==="
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

# 2. 作业吞吐量（需要Flink Metrics API）
echo "=== 吞吐量指标 ==="
curl -s http://localhost:8091/jobs/overview 2>/dev/null | jq '.jobs[] | {id: .jid, name: .name, state: .state}' || echo "无法获取作业指标"

# 3. 数据延迟检查
echo "=== 数据延迟检查 ==="
docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source_db -c "SELECT COUNT(*), MAX(event_time) FROM device_raw_data;" 2>/dev/null

echo "✅ 监控完成"
EOF

chmod +x ~/fluss_scenarios/performance_monitor.sh
```

---

## 🎯 实操示例

### 完整的测试流程
```bash
# 1. 启动环境
docker-compose up -d
sleep 30

# 2. 执行监控
~/fluss_scenarios/monitor.sh

# 3. 运行场景
~/fluss_scenarios/run_scenario1.sh

# 4. 实时监控数据
~/fluss_scenarios/performance_monitor.sh

# 5. 如遇故障，执行诊断
~/fluss_scenarios/diagnose.sh

# 6. 如需数据补货
~/fluss_scenarios/data_recovery.sh
```

### 手动测试数据变更
```sql
-- 手动插入测试数据
INSERT INTO insert_to_postgres_source VALUES
('MANUAL_001', 235.5, 150.0, 45.2, 350.8, 0.96, 'A', 'H', CURRENT_TIMESTAMP);

-- 手动更新数据测试增量计算
UPDATE insert_to_postgres_source 
SET efficiency = 0.88, alert_level = 'M' 
WHERE device_id = 'MANUAL_001';

-- 手动删除数据测试
DELETE FROM insert_to_postgres_source WHERE device_id = 'MANUAL_001';
```

---

**🎉 现在您已经拥有了完整的自主测试和运维能力！**

**核心要点**:
- ✅ **分段执行**: 避免一次性执行大脚本
- ✅ **实时监控**: 持续观察数据流状态  
- ✅ **故障自愈**: 快速诊断和恢复能力
- ✅ **性能调优**: 根据业务需求优化参数 