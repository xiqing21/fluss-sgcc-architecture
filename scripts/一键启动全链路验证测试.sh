#!/bin/bash

# ğŸš€ SGCC Fluss ä¸€é”®å¯åŠ¨å…¨é“¾è·¯éªŒè¯æµ‹è¯•è„šæœ¬
# åŠŸèƒ½ï¼šç¯å¢ƒå¯åŠ¨ + ä¸šåŠ¡SQLæ‰§è¡Œ + å…¨é“¾è·¯æ•°æ®éªŒè¯ + å¢åˆ æ”¹æµ‹è¯• + æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
# ä½œè€…ï¼šAIåŠ©æ‰‹ & ç”¨æˆ·åä½œå¼€å‘
# ç‰ˆæœ¬ï¼šv1.0
# æ—¥æœŸï¼š$(date +%Y-%m-%d)

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æµ‹è¯•é…ç½®
TEST_START_TIME=$(date +%s)
TEST_REPORT_DIR="test-reports"
TEST_REPORT_FILE="$TEST_REPORT_DIR/full_test_report_$(date +%Y%m%d_%H%M%S).md"

# åˆ›å»ºæµ‹è¯•æŠ¥å‘Šç›®å½•
mkdir -p "$TEST_REPORT_DIR"

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
    echo "[INFO] $1" >> "$TEST_REPORT_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
    echo "[SUCCESS] $1" >> "$TEST_REPORT_FILE"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
    echo "[WARNING] $1" >> "$TEST_REPORT_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
    echo "[ERROR] $1" >> "$TEST_REPORT_FILE"
}

# æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
declare -A metrics
metrics[total_records_processed]=0
metrics[total_jobs_created]=0
metrics[total_test_scenarios]=0
metrics[successful_scenarios]=0
metrics[failed_scenarios]=0

# åˆå§‹åŒ–æµ‹è¯•æŠ¥å‘Š
init_test_report() {
    cat > "$TEST_REPORT_FILE" << EOF
# ğŸš€ SGCC Fluss å…¨é“¾è·¯éªŒè¯æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•å¼€å§‹æ—¶é—´**: $(date)  
**æµ‹è¯•ç¯å¢ƒ**: Docker Compose + Fluss 0.7.0 + Flink 1.20 + PostgreSQL  
**æµ‹è¯•ç‰ˆæœ¬**: v1.0  

---

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ

EOF
}

# ç¯å¢ƒå¯åŠ¨å‡½æ•°
start_environment() {
    log_info "ğŸŒŸ æ­¥éª¤1: å¯åŠ¨æµ‹è¯•ç¯å¢ƒ"
    
    # åœæ­¢ç°æœ‰ç¯å¢ƒ
    log_info "åœæ­¢ç°æœ‰ç¯å¢ƒ..."
    docker-compose down > /dev/null 2>&1
    
    # æ¸…ç†Dockerå·
    log_info "æ¸…ç†Dockerå·..."
    docker volume prune -f > /dev/null 2>&1
    
    # å¯åŠ¨ç¯å¢ƒ
    log_info "å¯åŠ¨Docker Composeç¯å¢ƒ..."
    if docker-compose up -d; then
        log_success "ç¯å¢ƒå¯åŠ¨æˆåŠŸ"
    else
        log_error "ç¯å¢ƒå¯åŠ¨å¤±è´¥"
        exit 1
    fi
    
    # ç­‰å¾…æœåŠ¡å°±ç»ª
    log_info "ç­‰å¾…æœåŠ¡å°±ç»ª..."
    sleep 45
    
    # éªŒè¯æœåŠ¡çŠ¶æ€
    log_info "éªŒè¯æœåŠ¡çŠ¶æ€..."
    local services=("coordinator-server-sgcc" "tablet-server-sgcc" "jobmanager-sgcc" "taskmanager-sgcc-1" "postgres-sgcc-source" "postgres-sgcc-sink")
    for service in "${services[@]}"; do
        if docker ps --format "table {{.Names}}" | grep -q "$service"; then
            log_success "âœ… $service æœåŠ¡æ­£å¸¸"
        else
            log_error "âŒ $service æœåŠ¡å¼‚å¸¸"
        fi
    done
}

# æ‰§è¡ŒSQLè„šæœ¬å‡½æ•°
execute_sql_script() {
    local script_file="$1"
    local description="$2"
    
    log_info "æ‰§è¡Œ: $description"
    
    local start_time=$(date +%s)
    
    if docker exec -i sql-client-sgcc /opt/flink/bin/sql-client.sh embedded < "$script_file"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log_success "âœ… $description å®Œæˆ (è€—æ—¶: ${duration}ç§’)"
        metrics[total_jobs_created]=$((metrics[total_jobs_created] + 1))
        return 0
    else
        log_error "âŒ $description å¤±è´¥"
        return 1
    fi
}

# æ•°æ®éªŒè¯å‡½æ•°
validate_data() {
    log_info "ğŸ” æ­¥éª¤2: æ•°æ®éªŒè¯"
    
    # éªŒè¯PostgreSQLæºæ•°æ®
    log_info "éªŒè¯PostgreSQLæºæ•°æ®..."
    local source_count=$(docker exec postgres-sgcc-source psql -U sgcc_user -d sgcc_source -t -c "SELECT COUNT(*) FROM electrical_data;" 2>/dev/null | tr -d ' ')
    if [[ "$source_count" -gt 0 ]]; then
        log_success "âœ… æºæ•°æ®è¡¨è®°å½•æ•°: $source_count"
        metrics[total_records_processed]=$((metrics[total_records_processed] + source_count))
    else
        log_warning "âš ï¸ æºæ•°æ®è¡¨æ— æ•°æ®"
    fi
    
    # éªŒè¯Flusså„å±‚æ•°æ®
    log_info "éªŒè¯Flussæ•°æ®æ¹–å„å±‚æ•°æ®..."
    
    # åˆ›å»ºéªŒè¯SQLè„šæœ¬
    cat > /tmp/validate_fluss_data.sql << 'EOF'
CREATE CATALOG fluss_catalog WITH (
    'type' = 'fluss',
    'bootstrap.servers' = 'coordinator-server-sgcc:9123'
);

USE CATALOG fluss_catalog;

-- éªŒè¯ODSå±‚
SELECT 'ODSå±‚è®°å½•æ•°' as layer, COUNT(*) as count FROM sgcc_ods.electrical_data_ods;

-- éªŒè¯DWDå±‚
SELECT 'DWDå±‚è®°å½•æ•°' as layer, COUNT(*) as count FROM sgcc_dwd.electrical_data_dwd;

-- éªŒè¯DWSå±‚
SELECT 'DWSå±‚è®°å½•æ•°' as layer, COUNT(*) as count FROM sgcc_dws.electrical_data_dws;

-- éªŒè¯ADSå±‚
SELECT 'ADSå±‚è®°å½•æ•°' as layer, COUNT(*) as count FROM sgcc_ads.alarm_intelligence_report;

-- éªŒè¯æœ€æ–°æ•°æ®
SELECT 'DWSæœ€æ–°æ•°æ®' as info, device_id, avg_current, max_power, record_time 
FROM sgcc_dws.electrical_data_dws 
ORDER BY record_time DESC LIMIT 5;
EOF
    
    # æ‰§è¡ŒéªŒè¯
    if execute_sql_script "/tmp/validate_fluss_data.sql" "Flussæ•°æ®æ¹–éªŒè¯"; then
        log_success "âœ… Flussæ•°æ®æ¹–éªŒè¯å®Œæˆ"
    else
        log_error "âŒ Flussæ•°æ®æ¹–éªŒè¯å¤±è´¥"
    fi
    
    # éªŒè¯PostgreSQLç›®æ ‡æ•°æ®
    log_info "éªŒè¯PostgreSQLç›®æ ‡æ•°æ®..."
    local target_count=$(docker exec postgres-sgcc-sink psql -U sgcc_user -d sgcc_target -t -c "SELECT COUNT(*) FROM alarm_intelligence_report;" 2>/dev/null | tr -d ' ')
    if [[ "$target_count" -gt 0 ]]; then
        log_success "âœ… ç›®æ ‡æ•°æ®è¡¨è®°å½•æ•°: $target_count"
        metrics[total_records_processed]=$((metrics[total_records_processed] + target_count))
    else
        log_warning "âš ï¸ ç›®æ ‡æ•°æ®è¡¨æ— æ•°æ®"
    fi
}

# ä¸šåŠ¡åœºæ™¯æµ‹è¯•å‡½æ•°
run_business_scenarios() {
    log_info "ğŸ¯ æ­¥éª¤3: ä¸šåŠ¡åœºæ™¯æµ‹è¯•"
    
    local scenarios=(
        "business-scenarios/åœºæ™¯1_é«˜é¢‘ç»´åº¦è¡¨æœåŠ¡.sql:åœºæ™¯1_é«˜é¢‘ç»´åº¦è¡¨æœåŠ¡"
        "business-scenarios/åœºæ™¯2_æ™ºèƒ½åŒæµJOIN.sql:åœºæ™¯2_æ™ºèƒ½åŒæµJOIN"
        "business-scenarios/åœºæ™¯3_æ—¶é—´æ—…è¡ŒæŸ¥è¯¢.sql:åœºæ™¯3_æ—¶é—´æ—…è¡ŒæŸ¥è¯¢"
        "business-scenarios/åœºæ™¯4_æŸ±çŠ¶æµä¼˜åŒ–.sql:åœºæ™¯4_æŸ±çŠ¶æµä¼˜åŒ–"
    )
    
    for scenario in "${scenarios[@]}"; do
        local script_file="${scenario%:*}"
        local description="${scenario#*:}"
        
        metrics[total_test_scenarios]=$((metrics[total_test_scenarios] + 1))
        
        if execute_sql_script "$script_file" "$description"; then
            metrics[successful_scenarios]=$((metrics[successful_scenarios] + 1))
        else
            metrics[failed_scenarios]=$((metrics[failed_scenarios] + 1))
        fi
        
        # åœºæ™¯é—´æš‚åœ
        sleep 10
    done
}

# å¢åˆ æ”¹æµ‹è¯•å‡½æ•°
run_crud_operations() {
    log_info "ğŸ”„ æ­¥éª¤4: å¢åˆ æ”¹æ“ä½œæµ‹è¯•"
    
    # åˆ›å»ºå¢åˆ æ”¹æµ‹è¯•è„šæœ¬
    cat > /tmp/crud_operations.sql << 'EOF'
-- æ’å…¥æµ‹è¯•æ•°æ®
INSERT INTO electrical_data (device_id, voltage, current, power, temperature, humidity, location, status, record_time) 
VALUES 
  ('TEST_DEVICE_001', 220.5, 15.2, 3350.0, 25.0, 60.0, 'TEST_LOCATION', 'NORMAL', NOW()),
  ('TEST_DEVICE_002', 218.3, 14.8, 3230.0, 26.5, 58.0, 'TEST_LOCATION', 'NORMAL', NOW()),
  ('TEST_DEVICE_003', 225.1, 16.1, 3625.0, 24.0, 62.0, 'TEST_LOCATION', 'NORMAL', NOW());

-- æ›´æ–°æµ‹è¯•æ•°æ®
UPDATE electrical_data SET status = 'ALERT', temperature = 35.0 WHERE device_id = 'TEST_DEVICE_001';

-- åˆ é™¤æµ‹è¯•æ•°æ®
DELETE FROM electrical_data WHERE device_id = 'TEST_DEVICE_003';
EOF
    
    # æ‰§è¡Œå¢åˆ æ”¹æ“ä½œ
    log_info "æ‰§è¡Œå¢åˆ æ”¹æ“ä½œ..."
    if docker exec -i postgres-sgcc-source psql -U sgcc_user -d sgcc_source < /tmp/crud_operations.sql; then
        log_success "âœ… å¢åˆ æ”¹æ“ä½œæ‰§è¡ŒæˆåŠŸ"
        
        # ç­‰å¾…CDCåŒæ­¥
        log_info "ç­‰å¾…CDCåŒæ­¥..."
        sleep 30
        
        # éªŒè¯åŒæ­¥ç»“æœ
        log_info "éªŒè¯CDCåŒæ­¥ç»“æœ..."
        validate_data
        
    else
        log_error "âŒ å¢åˆ æ”¹æ“ä½œæ‰§è¡Œå¤±è´¥"
    fi
}

# æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡å‡½æ•°
calculate_metrics() {
    log_info "ğŸ“ˆ æ­¥éª¤5: æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡"
    
    local test_end_time=$(date +%s)
    local total_duration=$((test_end_time - TEST_START_TIME))
    
    # è·å–Flinkä½œä¸šä¿¡æ¯
    log_info "è·å–Flinkä½œä¸šä¿¡æ¯..."
    local running_jobs=$(docker exec jobmanager-sgcc /opt/flink/bin/flink list | grep -c "RUNNING" || echo "0")
    
    # è®¡ç®—ååé‡
    local throughput=0
    if [[ $total_duration -gt 0 ]]; then
        throughput=$((metrics[total_records_processed] / total_duration))
    fi
    
    # è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
    cat >> "$TEST_REPORT_FILE" << EOF

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»è€—æ—¶ | ${total_duration}ç§’ |
| å¤„ç†è®°å½•æ•° | ${metrics[total_records_processed]} |
| åˆ›å»ºä½œä¸šæ•° | ${metrics[total_jobs_created]} |
| å½“å‰è¿è¡Œä½œä¸šæ•° | ${running_jobs} |
| æµ‹è¯•åœºæ™¯æ•° | ${metrics[total_test_scenarios]} |
| æˆåŠŸåœºæ™¯æ•° | ${metrics[successful_scenarios]} |
| å¤±è´¥åœºæ™¯æ•° | ${metrics[failed_scenarios]} |
| æ•°æ®ååé‡ | ${throughput} è®°å½•/ç§’ |
| æˆåŠŸç‡ | $(( metrics[successful_scenarios] * 100 / metrics[total_test_scenarios] ))% |

## ğŸ”§ ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

EOF
    
    # è·å–å®¹å™¨èµ„æºä½¿ç”¨æƒ…å†µ
    log_info "è·å–å®¹å™¨èµ„æºä½¿ç”¨æƒ…å†µ..."
    docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}" >> "$TEST_REPORT_FILE"
    
    # è¾“å‡ºåˆ°ç»ˆç«¯
    log_success "ğŸ“Š æµ‹è¯•å®Œæˆç»Ÿè®¡ï¼š"
    log_success "  - æ€»è€—æ—¶: ${total_duration}ç§’"
    log_success "  - å¤„ç†è®°å½•æ•°: ${metrics[total_records_processed]}"
    log_success "  - åˆ›å»ºä½œä¸šæ•°: ${metrics[total_jobs_created]}"
    log_success "  - æˆåŠŸåœºæ™¯æ•°: ${metrics[successful_scenarios]}/${metrics[total_test_scenarios]}"
    log_success "  - æ•°æ®ååé‡: ${throughput} è®°å½•/ç§’"
}

# æ¸…ç†å‡½æ•°
cleanup() {
    log_info "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
    rm -f /tmp/validate_fluss_data.sql /tmp/crud_operations.sql
}

# ä¸»å‡½æ•°
main() {
    echo -e "${BLUE}ğŸš€ SGCC Fluss ä¸€é”®å¯åŠ¨å…¨é“¾è·¯éªŒè¯æµ‹è¯•${NC}"
    echo -e "${BLUE}=================================================${NC}"
    
    # åˆå§‹åŒ–æµ‹è¯•æŠ¥å‘Š
    init_test_report
    
    # æ‰§è¡Œæµ‹è¯•æµç¨‹
    start_environment
    validate_data
    run_business_scenarios
    run_crud_operations
    calculate_metrics
    cleanup
    
    # å®Œæˆä¿¡æ¯
    log_success "ğŸ‰ å…¨é“¾è·¯éªŒè¯æµ‹è¯•å®Œæˆï¼"
    log_success "ğŸ“„ è¯¦ç»†æŠ¥å‘Š: $TEST_REPORT_FILE"
    
    echo -e "${GREEN}ğŸ‰ æµ‹è¯•å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: $TEST_REPORT_FILE${NC}"
}

# æ•è·ä¸­æ–­ä¿¡å·
trap cleanup EXIT

# æ‰§è¡Œä¸»å‡½æ•°
main "$@" 